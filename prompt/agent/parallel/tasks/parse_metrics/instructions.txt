0. Resolve the working directory robustly: if os.path.basename(os.getcwd()) == "working" use os.getcwd(), otherwise use os.path.join(os.getcwd(), "working").
1. Load experiment output data from the working directory. The output filename follows the pattern {experiment_name}_data.npy. Search in this order:
   - glob for *_data.npy in working_dir (preferred pattern)
   - glob for experiment_data_*.npy in working_dir and os.getcwd()
   - os.path.join(os.getcwd(), "artifacts", "final", "output.npy")
   - os.path.join(working_dir, "artifacts", "final", "output.npy")
   - glob for output*.npy in working_dir and os.getcwd()
   - a recursive search under working_dir only (do not recurse os.getcwd) for any .npy file, with preference order: *_data.npy, experiment_data*.npy, then output*.npy, then any other .npy
   - If the prompt context provides a list of "Available .npy files", prefer only those paths that live under working_dir or match *_data.npy/experiment_data*.npy/output*.npy.
   - Ignore any .npy under .pydeps, .venv, site-packages, dist-packages, or similar dependency/data directories.
   If nothing is found, raise FileNotFoundError listing the attempted paths and patterns.
2. Extract metrics for each dataset. Load the .npy file and inspect its top-level keys to discover dataset/case names (e.g., the keys of the loaded dictionary). Use those internal keys as the dataset names â€” do NOT use the .npy filename stem as the dataset name, since it reflects the experiment name, not the actual datasets tested. If the loaded data is not a dictionary or has no meaningful sub-keys, use the experiment name as a fallback. Make sure to refer to the original code to understand the structure of the data.
3. Always print the name of the dataset before printing the metrics
4. Always print the name of the metric before printing the value by specifying the metric name clearly. Avoid vague terms like 'run1,' 'result,' or 'output.' Instead, use precise labels such as 'execution time (s),' 'throughput (GFLOPS),' 'convergence error,' 'final accuracy,' etc.
5. You only need to print the best or final value for each metric for each dataset
6. DO NOT CREATE ANY PLOTS

CRITICAL - Handling missing or empty metrics:
  - NEVER output fallback/placeholder metrics like "parsed_records_count", "record_count", "num_records", or similar metadata when no real performance metrics are found.
  - If no meaningful performance metrics (e.g., MFLOPS, GFLOPS, execution time, latency, throughput, convergence error, accuracy, etc.) can be extracted from the data, raise a ValueError with a descriptive message explaining what was expected vs. what was found.
  - Do NOT print "metric: ... value: 0" as a fallback when data is empty or unparseable. This causes the system to incorrectly believe metrics were successfully extracted.
  - Examples of INVALID fallback outputs (DO NOT DO THIS):
      print("metric: parsed_records_count value: 0")  # WRONG
      print("metric: record_count value: 0")          # WRONG
      print("metric: num_samples value: 0")           # WRONG
  - Instead, if no valid metrics are found, raise an exception:
      raise ValueError(f"No valid performance metrics found in {npy_path}. Data structure: {type(loaded)}, keys: {loaded.keys() if hasattr(loaded, 'keys') else 'N/A'}")

Important code structure requirements:
  - Do NOT put any execution code inside 'if __name__ == "__main__":' block. Do not use 'if __name__ == "__main__":' at all.
  - All code should be at the global scope or in functions that are called from the global scope
  - The script should execute immediately when run, without requiring any special entry point
