# path to the task data directory
data_dir: "data"
preprocess_data: False

goal: null
eval: null

log_dir: logs
workspace_dir: workspaces

# whether to copy the data to the workspace directory (otherwise it will be symlinked)
# copying is recommended to prevent the agent from accidentally modifying the original data
copy_data: True

exp_name: run # a random experiment name will be generated if not provided

# settings for code execution
exec:
  timeout: 3600
  agent_file_name: runfile.py
  format_tb_ipython: False
  language: cpp
  cpp_compiler: g++
  cpp_compile_flags: [
    -O2,
    -std=c++20,
    -fopenmp,
    -I/home/users/takanori.kotama/miniconda3/envs/ai_scientist/include,
    -I/home/users/takanori.kotama/miniconda3/envs/ai_scientist/libtorch/include/torch/csrc/api/include,
    -I/home/users/takanori.kotama/miniconda3/envs/ai_scientist/targets/x86_64-linux/include,
    -I/home/users/takanori.kotama/miniconda3/envs/ai_scientist/include/python3.11,
    -I/home/users/takanori.kotama/miniconda3/envs/ai_scientist/include/eigen3,
    -I/home/users/takanori.kotama/miniconda3/envs/ai_scientist/include/xtensor,
    -I/home/users/takanori.kotama/miniconda3/envs/ai_scientist/include/xtl,
    -I/home/users/takanori.kotama/miniconda3/envs/ai_scientist/include/matplotlibcpp,
    -I/home/users/takanori.kotama/miniconda3/envs/ai_scientist/external/libtorch-cu124/include,
    -I/home/users/takanori.kotama/miniconda3/envs/ai_scientist/external/libtorch-cu124/include/torch/csrc/api/include,
    -L/home/users/takanori.kotama/miniconda3/envs/ai_scientist/lib,
    -L/home/users/takanori.kotama/miniconda3/envs/ai_scientist/external/libtorch-cu124/lib,
    -L/home/users/takanori.kotama/miniconda3/envs/ai_scientist/libtorch/lib,
    -L/usr/lib64,
    -L/lib64,
    -lcnpy,
    -lcudart,
    -lmkl_rt,
    -lcublas,
    -lopenblas,
    -lpython3.11,
    -lpthread,
    -ltorch,
    -ltorch_cpu,
    -ltorch_cuda,
    -lc10,
    -lnuma,
    -ldl,
    -lspdlog,
    -lfmt,
    -lm,
    -lhwloc,
    -lpthread,
    -lstdc++fs
  ]
  phase_mode: split
  singularity_image: null
  container_runtime: null
  workspace_mount: "/workspace"
  writable_tmpfs: true
  container_overlay: null
  container_extra_args: []
  per_worker_sif: true
  keep_sandbox: false
  use_fakeroot: true
  writable_mode: auto

generate_report: True
# LLM settings for final report from journal
report:
  model: gpt-4o
  temp: 1.0

prompt_adapter:
  model: gpt-4o
  temp: 0.2
  max_tokens: 4096

experiment:
  num_syn_datasets: 1
  dataset_source: local

debug:
  stage4: False

# agent hyperparams
agent:
  type: parallel
  num_workers: 4
  stages:
    stage1_max_iters: 20
    stage2_max_iters: 12
    stage3_max_iters: 12
    stage4_max_iters: 18
  # how many improvement iterations to run
  steps: 5 # if stage-specific max_iters are not provided, the agent will use this value for all stages
  # whether to instruct the agent to use CV (set to 1 to disable)
  k_fold_validation: 1
  multi_seed_eval:
    num_seeds: 3 # should be the same as num_workers if num_workers < 3. Otherwise, set it to be 3.
  # whether to instruct the agent to generate a prediction function
  expose_prediction: False
  # whether to provide the agent with a preview of the data
  data_preview: False
  # persona that is injected into prompts (replace occurrences of "AI researcher")
  role_description: "Domain-neutral research engineer"

  # LLM settings for coding
  code:
    model: gpt-4o
    temp: 1.0
    max_tokens: null

  # LLM settings for evaluating program output / tracebacks
  feedback:
    model: gpt-4o
    # gpt-4o
    temp: 0.5
    max_tokens: null

  vlm_feedback:
    model: gpt-4o
    temp: 0.5
    max_tokens: null

  search:
    max_debug_depth: 3
    debug_prob: 0.5
    num_drafts: 3

  # Options for summarizing findings and selecting the best node
  # If not specified, the default behavior will be used.

  summary:
    model: gpt-4o
    temp: 0.3

  select_node:
    model: gpt-4o
    temp: 0.3
